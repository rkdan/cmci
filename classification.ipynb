{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rkd/Research/Monke/cmci/venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import json\n",
    "from tqdm import trange\n",
    "from functools import lru_cache\n",
    "import matplotlib.pyplot as plt; plt.ion()\n",
    "\n",
    "from umap import UMAP\n",
    "from librosa.feature import mfcc\n",
    "from librosa.core.spectrum import stft\n",
    "\n",
    "from scipy.io import wavfile as wav\n",
    "from scipy.signal import spectrogram\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SR = 44100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LOC = 'data/Calls for ML/labelled_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Files:\n",
    "    \"\"\"Class to house file paths for labelled data.\n",
    "    \"\"\"\n",
    "    data_loc = 'data/Calls for ML/'\n",
    "\n",
    "    # create symlinks so that all the data can be seen from labelled_data\n",
    "    lb_data_loc = 'data/Calls for ML/labelled_data/'\n",
    "\n",
    "    state_dict = 'data/Calls for ML/simple_rnn_sd.pth'\n",
    "\n",
    "    ml_test = 'ML_Test.wav'\n",
    "    labels_file = 'Calls_ML.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LOC = 'data/Calls for ML/labelled_data/'\n",
    "\n",
    "class AudioDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, device='cpu'):\n",
    "        self.audio = {\n",
    "            f.replace('.wav', ''): self.load_audio(Files.lb_data_loc + f).to(device) for f in os.listdir(Files.lb_data_loc) if '.wav' in f\n",
    "        }\n",
    "        self.audio_lens = {k: (len(a), len(a)/SR) for k, a in self.audio.items()}\n",
    "\n",
    "        calls = pd.read_excel(os.path.join(Files.lb_data_loc, Files.labels_file))\n",
    "        calls = calls.loc[(calls.Call_Type != 'interference'), ['File', 'Call_Type', 'Start', 'End']]\n",
    "        calls = calls.loc[~calls.Call_Type.isna(), ['File', 'Call_Type', 'Start', 'End']]\n",
    "        # calls['File'] = 'Calls_ML'\n",
    "        calls.columns = calls.columns.str.lower()\n",
    "\n",
    "\n",
    "        calls_shaldon = pd.read_excel(os.path.join(Files.lb_data_loc, 'Shaldon_Training_Labels.xlsx'))\n",
    "        calls_shaldon = calls_shaldon.loc[~calls_shaldon.Call_Type.isna(), ['File', 'Call_Type', 'Start', 'End']]\n",
    "        calls_shaldon['File'] = 'Shaldon_Combined'\n",
    "        calls_shaldon.columns = calls_shaldon.columns.str.lower()\n",
    "\n",
    "        calls_blackpool = pd.read_excel(os.path.join(Files.lb_data_loc, 'Blackpool_Labels.xlsx'))\n",
    "        calls_blackpool = calls_blackpool.loc[~calls_blackpool.Call_Type.isna(), ['File', 'Call_Type', 'Start', 'End']]\n",
    "        calls_blackpool['File'] = 'Blackpool_Combined_FINAL'\n",
    "        calls_blackpool.columns = calls_blackpool.columns.str.lower()\n",
    "\n",
    "        labels = pd.concat([calls, calls_shaldon, calls_blackpool], axis=0).reset_index(drop=True)\n",
    "\n",
    "        labels.loc[labels.call_type.isin(['Phee', 'Trill', 'Whistle']), 'call_type'] = 'LongCalls'\n",
    "        labels.loc[labels.call_type.isin(['Cheep', 'Chuck', 'Tsit']), 'call_type'] = 'ShortCalls'\n",
    "\n",
    "        # Remove calls that have length 0\n",
    "        self.labels = labels.loc[labels.end - labels.start > 0].reset_index(drop=True)\n",
    "\n",
    "        self.X = np.vstack([\n",
    "            self.process_file(*self.labels.loc[i, ['file', 'start', 'end']], sr=SR)\n",
    "            for i in self.labels.index\n",
    "        ])\n",
    "\n",
    "        y = np.array(self.labels.call_type, dtype=str)\n",
    "        self.le = LabelEncoder()\n",
    "        self.le.fit(y)\n",
    "        self.y_transformed = self.le.transform(y)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1\n",
    "    \n",
    "    \n",
    "    def load_audio(self, file_path):\n",
    "        sr, audio = self.load_audio_file(file_path)\n",
    "        audio = torchaudio.functional.resample(torch.tensor(audio), sr, SR)\n",
    "        return audio\n",
    "    \n",
    "    def process_file(self, f, start, end, sr, n_fft_prop=1/3):\n",
    "        a = self.load_audio(os.path.join(DATA_LOC, f + '.wav'))[int(start * sr):int(end * sr)].numpy()\n",
    "        # S = spectrogram(a, nperseg=len(a)//3, noverlap=len(a)//12, fs=sr)[-1]\n",
    "        S = np.abs(stft(a,\n",
    "            n_fft=int(len(a) * n_fft_prop),\n",
    "            hop_length=int(len(a) * n_fft_prop/2\n",
    "        )))\n",
    "        mel_features = mfcc(S=S, n_mfcc=20)\n",
    "        mel_features = (mel_features - mel_features.mean()) / (mel_features.std() + 1e-6)\n",
    "\n",
    "        features = np.hstack([\n",
    "            mel_features.reshape(-1),\n",
    "            self.additional_features(start, end)\n",
    "        ])\n",
    "        return features\n",
    "\n",
    "    def additional_features(self, start, end):\n",
    "        duration = end - start\n",
    "        additional_features = np.hstack([\n",
    "            duration,\n",
    "        ])\n",
    "        return additional_features\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    @lru_cache(maxsize=100)\n",
    "    def load_audio_file(filename):\n",
    "        sr, audio = wav.read(filename)\n",
    "        if len(audio.shape) == 2:\n",
    "            audio = audio[:, 0]  # take the first channel\n",
    "        audio = audio.astype('f')/1000  # scale values down by 1000.\n",
    "        return sr, audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AudioDataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3302, 141)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(torch.nn.Module):\n",
    "    def __init__(self, num_classes, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.linear2(x)\n",
    "        x = nn.Softmax(dim=1)(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset.X, dataset.y_transformed, test_size=0.15, random_state=1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataset and data loader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CallsDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.n = X.shape[0]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "train_dataset = CallsDataset(X_train, y_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "test_dataset = CallsDataset(X_test, y_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 1.663270\n",
      "Epoch 10: 1.515315\n",
      "Epoch 20: 1.442126\n",
      "Epoch 30: 1.322674\n",
      "Epoch 40: 1.566721\n",
      "Epoch 50: 1.295004\n",
      "Epoch 60: 1.276551\n",
      "Epoch 70: 1.277666\n",
      "Epoch 80: 1.623746\n",
      "Epoch 90: 1.411573\n"
     ]
    }
   ],
   "source": [
    "model = Classifier(num_classes=len(dataset.le.classes_), input_size=dataset.X.shape[1], hidden_size=128)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# train the model\n",
    "for epoch in range(100):\n",
    "    for X_batch, y_batch in train_dataloader:\n",
    "        y_pred = model(X_batch.float())\n",
    "        l = loss(y_pred, y_batch)\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}: {l.item():.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.401187\n",
      "88.10% accuracy\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(torch.from_numpy(X_test).float())\n",
    "    l = loss(y_pred, torch.from_numpy(y_test))\n",
    "    print(f'Loss: {l.item():.6f}')\n",
    "    y_pred = y_pred.argmax(dim=1).numpy()\n",
    "    print(f'{100*(y_pred == y_test).sum() / len(y_test):.2f}% accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Jagged Trill', 'LongCalls', 'Moan', 'Resonate', 'Resonating Note',\n",
       "       'ShortCalls', 'Sneeze', 'Whistle '], dtype='<U15')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "call_type\n",
       "ShortCalls         1264\n",
       "LongCalls           843\n",
       "Resonating Note     667\n",
       "Resonate            285\n",
       "Moan                100\n",
       "Jagged Trill         96\n",
       "Whistle              24\n",
       "Sneeze               23\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get unique labels counts from dataset.labels\n",
    "dataset.labels.call_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  82,  705,   84,  247,  563, 1088,   18,   19])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
